{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from math import ceil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les chemins de fichiers\n",
    "corpus_file = r'D:\\etude\\M2\\RI\\nfcorpus\\nfcorpus\\corpus.jsonl'\n",
    "queries_file = r'D:\\etude\\M2\\RI\\nfcorpus\\nfcorpus\\queries.jsonl'\n",
    "\n",
    "# Charger les corpus\n",
    "corpus_df = pd.read_json(corpus_file, lines=True)\n",
    "queries_df = pd.read_json(queries_file, lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de documents : 3633\n",
      "Nombre de requêtes : 3237\n"
     ]
    }
   ],
   "source": [
    "# Affichage du nombre de documents et de requêtes\n",
    "print(f\"Nombre de documents : {len(corpus_df)}\")\n",
    "print(f\"Nombre de requêtes : {len(queries_df)}\")\n",
    "\n",
    "# Charger les stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_file(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read().lower()  # Conversion en minuscules\n",
    "    \n",
    "    # Suppression de la ponctuation\n",
    "    content = re.sub(r'[^\\w\\s]', '', content)\n",
    "    tokens = word_tokenize(content)  # Tokenisation\n",
    "    \n",
    "    # Filtrage des stop words\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Enregistrement des tokens filtrés \n",
    "    with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "        f_out.write('\\n'.join(filtered_tokens))\n",
    "    \n",
    "    # Affichage des tokens filtrés\n",
    "    print(f\"Tokens après filtrage (stopwords) pour {input_file}: {filtered_tokens[:10]}...\")  # Afficher les 10 premiers tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de stemming\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Fonction pour effectuer le stemming et enregistrer le résultat\n",
    "def stemming_tokens(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        tokens = f.read().splitlines()  # Lecture des tokens ligne par ligne\n",
    "    \n",
    "    # Application du stemming\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Enregistrement des tokens stemmés dans un fichier de sortie\n",
    "    with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "        f_out.write('\\n'.join(stemmed_tokens))\n",
    "    \n",
    "    # Affichage des résultats\n",
    "    print(f\"Tokens après stemming pour {input_file}: {stemmed_tokens[:10]}...\")  # Afficher les 10 premiers tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de lemmatisation\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Fonction pour lemmatiser les tokens d'un fichier et enregistrer le résultat\n",
    "def lemmatize_tokens(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        tokens = f.read().splitlines()  # Lecture des tokens ligne par ligne\n",
    "    \n",
    "    # Application de la lemmatisation\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Enregistrement des tokens lemmatisés dans un fichier de sortie\n",
    "    with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "        f_out.write('\\n'.join(lemmatized_tokens))\n",
    "    \n",
    "    # Affichage des résultats\n",
    "    print(f\"Tokens après lemmatisation pour {input_file}: {lemmatized_tokens[:10]}...\")  # Afficher les 10 premiers tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tfidf(corpus_file, output_excel_file):\n",
    "    with open(corpus_file, 'r', encoding='utf-8') as f:\n",
    "        documents = f.read().splitlines()  # Lire les documents\n",
    "\n",
    "    # Calcul du TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    \n",
    "    # Conversion de la matrice TF-IDF en DataFrame\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), \n",
    "                            columns=vectorizer.get_feature_names_out(),  # Les termes dans les colonnes\n",
    "                            index=[f\"Document {i+1}\" for i in range(len(documents))])  # Les documents dans les lignes\n",
    "\n",
    "    # Découper les colonnes en plusieurs groupes si nécessaire\n",
    "    num_cols = tfidf_df.shape[1]\n",
    "    max_cols_per_sheet = 16384  # Limite des colonnes par feuille Excel\n",
    "    num_sheets = ceil(num_cols / max_cols_per_sheet)\n",
    "\n",
    "    # Écrire dans un fichier Excel avec plusieurs feuilles si nécessaire\n",
    "    with pd.ExcelWriter(output_excel_file) as writer:\n",
    "        for i in range(num_sheets):\n",
    "            start_col = i * max_cols_per_sheet\n",
    "            end_col = min((i + 1) * max_cols_per_sheet, num_cols)\n",
    "            sheet_df = tfidf_df.iloc[:, start_col:end_col]\n",
    "            sheet_name = f\"Sheet{i+1}\"\n",
    "            sheet_df.to_excel(writer, sheet_name=sheet_name)\n",
    "\n",
    "    # Affichage du résultat dans la console (optionnel)\n",
    "    print(f\"TF-IDF matrix saved to {output_excel_file}\")\n",
    "    print(tfidf_df.head())  # Affiche les 5 premières lignes pour vérification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.85, min_df=2)  # Ignore les termes apparaissant dans plus de 85% des documents, ou dans moins de 2 documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer le TF-IDF et l'exporter dans un fichier Excel lisible\n",
    "calculate_tfidf('corpus.txt', 'tfidf_results.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens après filtrage (stopwords) pour corpus.txt: ['med10statin', 'use', 'breast', 'cancer', 'survival', 'nationwide', 'cohort', 'study', 'finlandrecent', 'studies']...\n",
      "Tokens après filtrage (stopwords) pour queries.txt: ['plain3breast', 'cancer', 'cells', 'feed', 'cholesterolurl', 'httpnutritionfactsorg20150714breastcancercellsfeedoncholesterol', 'plain4using', 'diet', 'treat', 'asthma']...\n"
     ]
    }
   ],
   "source": [
    "# Appel des fonctions pour tokeniser, stemmer et lemmatiser\n",
    "tokenize_file('corpus.txt', 'corpus_tokens.txt')\n",
    "tokenize_file('queries.txt', 'queries_tokens.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens après stemming pour corpus_tokens.txt: ['med10statin', 'use', 'breast', 'cancer', 'surviv', 'nationwid', 'cohort', 'studi', 'finlandrec', 'studi']...\n",
      "Tokens après stemming pour queries_tokens.txt: ['plain3breast', 'cancer', 'cell', 'feed', 'cholesterolurl', 'httpnutritionfactsorg20150714breastcancercellsfeedoncholesterol', 'plain4us', 'diet', 'treat', 'asthma']...\n"
     ]
    }
   ],
   "source": [
    "stemming_tokens('corpus_tokens.txt', 'corpus_stemmed_tokens.txt')\n",
    "stemming_tokens('queries_tokens.txt', 'queries_stemmed_tokens.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens après lemmatisation pour corpus_tokens.txt: ['med10statin', 'use', 'breast', 'cancer', 'survival', 'nationwide', 'cohort', 'study', 'finlandrecent', 'study']...\n",
      "Tokens après lemmatisation pour queries_tokens.txt: ['plain3breast', 'cancer', 'cell', 'feed', 'cholesterolurl', 'httpnutritionfactsorg20150714breastcancercellsfeedoncholesterol', 'plain4using', 'diet', 'treat', 'asthma']...\n"
     ]
    }
   ],
   "source": [
    "lemmatize_tokens('corpus_tokens.txt', 'corpus_lemmatized_tokens.txt')\n",
    "lemmatize_tokens('queries_tokens.txt', 'queries_lemmatized_tokens.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix:\n",
      "         00  000  000006073  00001  0001  0002  0003  0004  0005  0006  ...  \\\n",
      "0  0.000000  0.0        0.0    0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
      "1  0.047673  0.0        0.0    0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
      "2  0.000000  0.0        0.0    0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
      "3  0.000000  0.0        0.0    0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
      "4  0.000000  0.0        0.0    0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
      "\n",
      "    μl   μm  μmol   μu  σ7pbdes  σoh  σpbde5  σttr   χ2   ⁶m  \n",
      "0  0.0  0.0   0.0  0.0      0.0  0.0     0.0   0.0  0.0  0.0  \n",
      "1  0.0  0.0   0.0  0.0      0.0  0.0     0.0   0.0  0.0  0.0  \n",
      "2  0.0  0.0   0.0  0.0      0.0  0.0     0.0   0.0  0.0  0.0  \n",
      "3  0.0  0.0   0.0  0.0      0.0  0.0     0.0   0.0  0.0  0.0  \n",
      "4  0.0  0.0   0.0  0.0      0.0  0.0     0.0   0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 33161 columns]\n"
     ]
    }
   ],
   "source": [
    "# Calculer le TF-IDF et afficher les résultats\n",
    "calculate_tfidf('corpus.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "introduction-TAL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
