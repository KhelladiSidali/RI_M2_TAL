{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7215c868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utilisateur\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer  # Ajout de PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e820eeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_tfidf(corpus):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    return tfidf_matrix, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbf63eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Définir les chemins de fichiers\n",
    "corpus_file = r'D:\\etude\\M2\\RI\\nfcorpus\\nfcorpus\\corpus.jsonl'\n",
    "queries_file = r'D:\\etude\\M2\\RI\\nfcorpus\\nfcorpus\\queries.jsonl'\n",
    "dev_qrels_file = r'D:\\etude\\M2\\RI\\nfcorpus\\nfcorpus\\qrels\\dev.tsv'\n",
    "test_qrels_file = r'D:\\etude\\M2\\RI\\nfcorpus\\nfcorpus\\qrels\\test.tsv'\n",
    "train_qrels_file = r'D:\\etude\\M2\\RI\\nfcorpus\\nfcorpus\\qrels\\train.tsv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "267ae409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le corpus\n",
    "corpus_df = pd.read_json(corpus_file, lines=True)\n",
    "queries_df = pd.read_json(queries_file, lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc7191b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter le nombre de documents et de requêtes\n",
    "num_docs = len(corpus_df)\n",
    "num_queries = len(queries_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f35e3cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de documents : 3633\n",
      "Nombre de requêtes : 3237\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nombre de documents : {num_docs}\")\n",
    "print(f\"Nombre de requêtes : {num_queries}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ba97565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir en txt\n",
    "corpus_df.to_csv('corpus.txt', index=False, header=False)\n",
    "queries_df.to_csv('queries.txt', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ba4d913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des stop words\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d394af8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de tokenisation et nettoyage\n",
    "def tokenize_file(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:  # Spécifier l'encodage ici\n",
    "        content = f.read().lower()  # Conversion en minuscules\n",
    "        \n",
    "    # Suppression de la ponctuation\n",
    "    content = re.sub(r'[^\\w\\s]', '', content)\n",
    "    tokens = word_tokenize(content)  # Tokenisation\n",
    "    \n",
    "    # Filtrage des stop words\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Enregistrement des tokens filtrés \n",
    "    with open(output_file, 'w', encoding='utf-8') as f_out:  # Spécifier l'encodage ici aussi\n",
    "        f_out.write('\\n'.join(filtered_tokens))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3faf359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Résultats de tokenisation\n",
    "tokenize_file('corpus.txt', 'corpus_tokens.txt')\n",
    "tokenize_file('queries.txt', 'queries_tokens.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef1ccf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Utilisateur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Utilisateur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Télécharger les ressources NLTK si nécessaire\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c4d06b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba25af91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de stemming\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63e06ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour lemmatiser les tokens d'un fichier et enregistrer le résultat\n",
    "def lemmatize_tokens(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:  # Spécifier l'encodage ici\n",
    "        tokens = f.read().splitlines()  # Lecture des tokens ligne par ligne\n",
    "    \n",
    "    # Application de la lemmatisation\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Enregistrement des tokens lemmatisés dans un fichier de sortie\n",
    "    with open(output_file, 'w', encoding='utf-8') as f_out:  # Spécifier l'encodage ici aussi\n",
    "        f_out.write('\\n'.join(lemmatized_tokens))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70a1cee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application de la lemmatisation aux fichiers\n",
    "lemmatize_tokens('corpus_tokens.txt', 'corpus_lemmatized_tokens.txt')\n",
    "lemmatize_tokens('queries_tokens.txt', 'queries_lemmatized_tokens.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "637435fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour effectuer le stemming et enregistrer le résultat\n",
    "def stemming_tokens(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        tokens = f.read().splitlines()  # Lecture des tokens ligne par ligne\n",
    "    \n",
    "    # Application du stemming\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Enregistrement des tokens stemmés dans un fichier de sortie\n",
    "    with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "        f_out.write('\\n'.join(stemmed_tokens))\n",
    "\n",
    "# Application du stemming aux fichiers\n",
    "stemming_tokens('corpus_tokens.txt', 'corpus_stemmed_tokens.txt')\n",
    "stemming_tokens('queries_tokens.txt', 'queries_stemmed_tokens.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cb073a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read().lower()  # Conversion en minuscules\n",
    "\n",
    "    # Suppression de la ponctuation\n",
    "    content = re.sub(r'[^\\w\\s]', '', content)\n",
    "    \n",
    "    # Tokenisation\n",
    "    tokens = word_tokenize(content)\n",
    "\n",
    "    # Retirer les stop words\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Stemming\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]  # Ajout du stemming\n",
    "\n",
    "    # Lemmatisation\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "    # Enregistrement des tokens traités dans un fichier\n",
    "    with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "        f_out.write('\\n'.join(lemmatized_tokens))\n",
    "\n",
    "    return filtered_tokens, stemmed_tokens, lemmatized_tokens  # Retourne les tokens filtrés, lemmatisés, et stemmés\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c3e661a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application du traitement aux fichiers\n",
    "corpus_filtered, corpus_stemmed, corpus_lemmatized = process_text('corpus.txt', 'corpus_processed.txt')\n",
    "queries_filtered, queries_stemmed, queries_lemmatized = process_text('queries.txt', 'queries_processed.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90827e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuster les options d'affichage de pandas\n",
    "pd.set_option('display.max_rows', None)       # Affiche toutes les lignes\n",
    "pd.set_option('display.max_columns', None)    # Affiche toutes les colonnes\n",
    "pd.set_option('display.width', None)          # Ajuste automatiquement la largeur\n",
    "pd.set_option('display.max_colwidth', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d7e0a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tfidf(corpus_file):\n",
    "    with open(corpus_file, 'r', encoding='utf-8') as f:\n",
    "        documents = f.read().splitlines()  # Lecture des documents\n",
    "    \n",
    "    # Utilisation de TfidfVectorizer pour calculer le TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    \n",
    "    # Conversion en DataFrame pour un affichage facile\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    \n",
    "    return tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fe0714",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_tfidf(\"corpus.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2170b1b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# On utilise le texte brut pour le calcul TF-IDF\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m corpus_tfidf, feature_names \u001b[38;5;241m=\u001b[39m calculate_tfidf([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(corpus_filtered)])\n",
      "Cell \u001b[1;32mIn[52], line 2\u001b[0m, in \u001b[0;36mcalculate_tfidf\u001b[1;34m(corpus_file)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_tfidf\u001b[39m(corpus_file):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(corpus_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      3\u001b[0m         documents \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplitlines()  \u001b[38;5;66;03m# Lecture des documents\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Utilisation de TfidfVectorizer pour calculer le TF-IDF\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:279\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(io_open)\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_modified_open\u001b[39m(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m         )\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# On utilise le texte brut pour le calcul TF-IDF\n",
    "corpus_tfidf, feature_names = calculate_tfidf([' '.join(corpus_filtered)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a296451",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
